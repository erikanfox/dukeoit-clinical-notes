{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in fake notes\n",
    "notes=pd.read_csv(\"~/data/fake_notes.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Filtering\n",
    "    The following cells filter out the kinds of notes that are guaranteed to ignore the SOAP format, such as covid vaccines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create indicators for note groups we want to remove'''\n",
    "\n",
    "\n",
    "# covid vaccines\n",
    "pattern = r'COVID-19 vaccine|COVID-19 Vaccine' #regular expression to identify if its a covid vaccine\n",
    "notes['covid_vacc'] = notes['note_text'].str.contains(pattern) #create indicator column for this type\n",
    "notes['covid_vacc']=np.where(notes[\"type\"]==\"COVID VACCINATION\",True,notes['covid_vacc']) #incorporate type COVID VACCINATION into indicator\n",
    "\n",
    "# OBGYN Discharge Summaries\n",
    "pattern = r'Obstetrics Postpartum Discharge Summary|Obstetrics Antepartum Discharge Summary|Discharge Summary'\n",
    "notes['ob_discharge'] = notes['note_text'].str.contains(pattern) \n",
    "\n",
    "# studies\n",
    "pattern = r'Study Title:'\n",
    "notes['study'] = notes['note_text'].str.contains(pattern) \n",
    "\n",
    "# newborn notes\n",
    "pattern = r'Duke University Newborn Nursery '\n",
    "notes['newborn'] = notes['note_text'].str.contains(pattern) \n",
    "\n",
    "#clincal skills\n",
    "pattern = r'Clinical Skills Foundation 1'\n",
    "notes['skills'] = notes['note_text'].str.contains(pattern) \n",
    "\n",
    "# only blood pressure\n",
    "pattern = r'Patient is here for blood pressure'\n",
    "notes['bp'] = notes['note_text'].str.contains(pattern) \n",
    "\n",
    "# HCC score\n",
    "pattern = r'HCC Score Gap Refresh'\n",
    "notes['gap'] = notes['note_text'].str.contains(pattern) \n",
    "\n",
    "# fetal monitoring\n",
    "pattern = r'Gest age [0-9]*w0d today|Fetal Monitoring|Fetal Heart Monitoring|Procedure Note NST|Fetal heart monitoring|EFM Monitoring|Fetal Tracing Review|Tracing Review|Intrapartum Monitoring'\n",
    "notes['note_text'].str.contains(pattern).sum()\n",
    "notes['gest'] = notes['note_text'].str.contains(pattern) \n",
    "\n",
    "# labor note\n",
    "pattern = r'Labor progress note|Brief Progress Note|Labor Progress Note'\n",
    "notes['labor'] = notes['note_text'].str.contains(pattern) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''removing notes that we know we wont find objective in'''\n",
    "\n",
    "notes[\"no_obj\"]=0 # instantiatie \"no objective\" column (will aggregate the above indicators)    \n",
    "                  # any note that has a positive value in any of the above indicators will have a\n",
    "                  # positve value in this columnnotes[\"no_obj\"]=np.where(notes[\"ob_discharge\"]==True,1,notes[\"no_obj\"])\n",
    "\n",
    "notes[\"no_obj\"]=np.where(notes[\"covid_vacc\"]==True,1,notes[\"no_obj\"])\n",
    "notes[\"no_obj\"]=np.where(notes[\"study\"]==True,1,notes[\"no_obj\"])\n",
    "notes[\"no_obj\"]=np.where(notes[\"newborn\"]==True,1,notes[\"no_obj\"])\n",
    "notes[\"no_obj\"]=np.where(notes[\"skills\"]==True,1,notes[\"no_obj\"])\n",
    "notes[\"no_obj\"]=np.where(notes[\"bp\"]==True,1,notes[\"no_obj\"])\n",
    "notes[\"no_obj\"]=np.where(notes[\"gap\"]==True,1,notes[\"no_obj\"])\n",
    "notes[\"no_obj\"]=np.where(notes[\"gest\"]==True,1,notes[\"no_obj\"])\n",
    "notes[\"no_obj\"]=np.where(notes[\"labor\"]==True,1,notes[\"no_obj\"])\n",
    "\n",
    "\n",
    "no_obj_notes=notes[notes[\"no_obj\"]==1].reset_index() # dataframe of notes that don't have objective\n",
    "notes_obj=notes[notes[\"no_obj\"]==0].reset_index() # dataframe of notes do have objective\n",
    "print(\"percent of notes with Objective: \"+str(notes_obj.shape[0]/notes.shape[0])) \n",
    "\n",
    "notes_obj=notes_obj.drop(columns=[\"split\",\"no_obj\",\"covid_vacc\",\n",
    "\"ob_discharge\",\"study\",\"newborn\",\"skills\",\"bp\",\"gap\",\"gest\",\"labor\",\n",
    "\"title\",\"specialty\",\"admission_service\",\n",
    "\"enc_type\",\t\"visit_reason\",\"index\",\"Unnamed: 0\"]) # keep only columns we need"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detection\n",
    "    Many of the notes treat Assessment and Plan as one component. We will deal with these notes first, and then filter these out, and then get the notes that have them seperate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some functions to help with detection\n",
    "\n",
    "def getMatches(col): \n",
    "    ''' creates list of regex matches and converts to dictionary'''\n",
    "    l=[]\n",
    "    for match in re.finditer(pattern, col):\n",
    "        l.append(match)\n",
    "    # sample l: [<re.Match object; span=(126, 152), match='History of Present Illness'>,\n",
    "    #  <re.Match object; span=(154, 169), match='FIRST_NAME_FULL'>]\n",
    "    return getMatchDict(l) # use following function to convert to dictionary\n",
    "    # l as a dictionary: {History of Present Illness':126,'FIRST_NAME_FULL':154}\n",
    "\n",
    "def getMatchDict(col):\n",
    "    '''convert list of matches to clean dictionary'''\n",
    "    if col is None:\n",
    "        return None\n",
    "    else:\n",
    "        dic=[]\n",
    "        for i in col:\n",
    "            dic.append((i[0],i.start()))\n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjective\n",
    "pattern = r'((Brief History of Present ,Illness:)|Chief Complaint|(?<![A-Z])S:|(concerns at this time include:)|subjective(?=[^a-zA-Z])+|Subjective(?=[^a-zA-Z])+|Interval Hx:|INTERVAL HISTORY|Interval Events|[Hh]istory of [Pp]resent [Ii]llness|Hospital Day:|Hosp day:|HISTORY OF PRESENT ILLNESS|HPI(?=[^a-zA-Z])+|FIRST_NAME_FULL|Interval History|SUBJECTIVE)'\n",
    "notes_obj['contains_subjective'] = notes_obj['note_text'].str.contains(pattern) \n",
    "print(\"subjective: \"+str(notes_obj['contains_subjective'].sum()/notes_obj.shape[0]))\n",
    "notes_obj['sub']= notes_obj['note_text'].apply(lambda x:getMatches(x))\n",
    "\n",
    "# Objective\n",
    "pattern = r'(Objective)|(?<![A-Z])O:|(?<![A-Z])PE:|Record of Physical Exams ,and NIH Stroke Scales|Physical exam:  Temp:|PHYSICAL EXAM     VITALS|OBJECTIVE|PHYSICAL EXAM AT DISCHARGE|Vital Sings last 24 hours:|Physical Exam   BP|Physical Exam   Pulse|Physical Exam   Temp:|Physical Exam at Discharge:|Physical exam:  BP|Physical Exam  Vitals|Record of Physical Exams and NIH Stroke Scales|EXAMINATION|Exam:|EXAM:|Current Vital Signs|Physical Exam[s]*:|Vital[s]*:|Vital signs:|Vital Signs:'\n",
    "notes_obj['contains_objective'] = notes_obj['note_text'].str.contains(pattern) \n",
    "print(\"objective: \"+str(notes_obj['contains_objective'].sum()/notes_obj.shape[0]))\n",
    "notes_obj['obj']= notes_obj['note_text'].apply(lambda x:getMatches(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment and Plan (together)\n",
    "# the percentage reported here represents the percent of total notes in which we found\n",
    "# assessment and plan grouped together\n",
    "\n",
    "pattern=r'(Assessment\\/Plan)|(Assessment\\/ Plan)|(?<![A-Z])A/P:|(Plan and Assessment)|(Assessment and Recommendations)|Assessment and plan:|Assessment &amp; Recommendations|(Assessment \\/Plan)|(Assessments and Plans)|(Assessment \\/ Plan)|(ASSESSMENT AND PLAN)|(Assessment &amp; Plan)|(Assessment and Plan)(?!\\))|(ASSESSMENT &amp; PLAN)|(ASSESSMENT \\/ RECOMMENDATIONS)|(ASSESSMENT \\/ PLAN)|(ASSESSMENT/COORDINATION OF CARE:)|(ASSESSMENT AND RECOMMENDATIONS)|(ASSESSMENT\\/PLAN)|(ASSESSMENT, RECOMMENDATIONS AND PLAN)'\n",
    "notes_obj['contains_assessplan'] = notes_obj['note_text'].str.contains(pattern) \n",
    "print(\"assessplan: \"+str(notes_obj['contains_assessplan'].sum()/notes_obj.shape[0]))\n",
    "notes_obj['assessplan']= notes_obj['note_text'].apply(lambda x:getMatches(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment and Plan (Separate)\n",
    "# the percentages reported here represent the percent of total notes in which we found\n",
    "# assessment or plan (respectively) where the section was not attached to the other\n",
    "\n",
    "# Assessment\n",
    "pattern=r'(?<!\\()Assessment(?!( and))|(?<!(SAFETY |ENERAL ))ASSESSMENT'\n",
    "notes_obj['contains_assess'] = notes_obj['note_text'].str.contains(pattern) \n",
    "print(\"assess: \"+str(notes_obj['contains_assess'].sum()/notes_obj.shape[0]))\n",
    "notes_obj['assess']= notes_obj['note_text'].apply(lambda x:getMatches(x))\n",
    "\n",
    "# Plan\n",
    "pattern=r'((?<!Specific )Plan:)|PLAN(?![A-Z])|Recommendations|COORDINATION OF CARE:|(Plan)(?! [A-Za-z])'\n",
    "notes_obj['contains_plan'] = notes_obj['note_text'].str.contains(pattern) \n",
    "print(\"plan: \"+str(notes_obj['contains_plan'].sum()/notes_obj.shape[0]))\n",
    "notes_obj['plan']= notes_obj['note_text'].apply(lambda x:getMatches(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is purely so we can calculate the percent of notes that have assessment and plan,\n",
    "# regardless if they are grouped together or not\n",
    "# (not used for extraction)\n",
    "\n",
    "pattern=r'(?<!\\()Assessment(?!( and))|(?<!(SAFETY |ENERAL ))ASSESSMENT|(?<![A-Z])A\\/P:|(Assessment\\/Plan)|(Assessment\\/ Plan)|(?<!\\()Assessment and plan(?!\\()|Assessment and plan:|Assessment &amp; Recommendations|(Plan and Assessment)|(Assessment and Recommendations)|(Assessment \\/Plan)|(Assessments and Plans)|(Assessment \\/ Plan)|(ASSESSMENT AND PLAN)|(Assessment &amp; Plan)|(Assessment and Plan)(?!\\))|(ASSESSMENT &amp; PLAN)|(ASSESSMENT \\/ RECOMMENDATIONS)|(ASSESSMENT \\/ PLAN)|(ASSESSMENT\\/COORDINATION OF CARE:)|(ASSESSMENT AND RECOMMENDATIONS)|(ASSESSMENT\\/PLAN)|(ASSESSMENT, RECOMMENDATIONS AND PLAN)'\n",
    "notes_obj['contains_a'] = notes_obj['note_text'].str.contains(pattern) \n",
    "print(\"a: \"+str(notes_obj['contains_a'].sum()/notes_obj.shape[0]))\n",
    "pattern=r'((?<!Specific )Plan:)|(?<!\\()Assessment and plan(?!\\()|(?<![A-Z])A\\/P:|(Plan)(?! [A-Za-z])|PLAN(?![A-Z])|COORDINATION OF CARE:|Recommendations|(Assessment\\/Plan)|(Assessment\\/ Plan)|Assessment and plan:|Assessment &amp; Recommendations|(Plan and Assessment)|(Assessment and Recommendations)|(Assessment \\/Plan)|(Assessments and Plans)|(Assessment \\/ Plan)|(ASSESSMENT AND PLAN)|(Assessment &amp; Plan)|(Assessment and Plan)(?!\\))|(ASSESSMENT &amp; PLAN)|(ASSESSMENT \\/ RECOMMENDATIONS)|(ASSESSMENT \\/ PLAN)|(ASSESSMENT\\/COORDINATION OF CARE:)|(ASSESSMENT AND RECOMMENDATIONS)|(ASSESSMENT\\/PLAN)|(ASSESSMENT, RECOMMENDATIONS AND PLAN)'\n",
    "notes_obj['contains_p'] = notes_obj['note_text'].str.contains(pattern) \n",
    "print(\"p: \"+str(notes_obj['contains_p'].sum()/notes_obj.shape[0]))\n",
    "notes_obj=notes_obj.drop(columns=[\"contains_a\",\"contains_p\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extraction\n",
    "Now that we have detected the keywords for each component, we will use them to separate the text.\n",
    "To extract, we will assume that all four components of SOAP are present, so we will start by removing notes that are missing any.\n",
    "\n",
    "Because some of the SOAP notes treat assessment and plan as one section, and therefore have three sections instead of four, we will handle them seperately.\n",
    "We will extract the sections from the notes that have the three section structure first, then handle the notes with four, and then append them together to get our final set of notes.\n",
    "\n",
    "For consistency, we will extract the notes into the four sections: Subjective, Objective, Assessment and Plan for both the \"three section\" and the four section notes. For the \"three section\" notes, the Assessment and Plan columns will just have the same value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are to be used with lambda to modify columns\n",
    "# these  are things that you may not normally need a function for, but are useful because we are operating on columns\n",
    "\n",
    "def replaceTitle(col,title):\n",
    "    '''easily rename a dictionary component'''\n",
    "    ind=col[1]\n",
    "    return (title,ind)\n",
    "\n",
    "def createOrderedDict(col):\n",
    "    dic=OrderedDict()\n",
    "    for i in col:\n",
    "        dic[i[0]]=i[1]\n",
    "    #return sorted(dic)\n",
    "    list=sorted(dic.items(), key=lambda x:x[1])\n",
    "    ans={}\n",
    "    for i in list:\n",
    "        ans[i[0]]=i[1]\n",
    "    return ans\n",
    "\n",
    "def getStartEnd(col,name):\n",
    "    index_sub=int(list(col).index(name))\n",
    "    next=index_sub+1\n",
    "\n",
    "    if next==len(col):\n",
    "        return (col[name],-1)\n",
    "    else:\n",
    "        return (col[name],list(col.items())[next][1])\n",
    "\n",
    "def getStart(col):\n",
    "    return col[0]\n",
    "\n",
    "def getEnd(col):\n",
    "    return col[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extraction of \"three section\" notes'''\n",
    "\n",
    "\n",
    "# create temporary DataFrame that has subjective, objective, and\n",
    "# assessment/plan treated as one group\n",
    "\n",
    "df=notes_obj[notes_obj[\"contains_objective\"]]\n",
    "print(\"percent of notes with objective: \"+str(df.shape[0]/notes_obj.shape[0]))\n",
    "df=df[df[\"contains_subjective\"]]\n",
    "print(\"percent of notes with objective and subjective: \"+str(df.shape[0]/notes_obj.shape[0]))\n",
    "df=df[df[\"contains_assessplan\"]]\n",
    "print(\"percent of notes with objective, subjective and assessment and plan treated as one group: \"+str(df.shape[0]/notes_obj.shape[0]))\n",
    "\n",
    "\n",
    "# If we got multiple matches for a header, we only want to look at the first match\n",
    "# This is because alternate headers are sometimes found within the section, but the\n",
    "# first appearance indicates the beginning\n",
    "# this code extracts the first item in the match dictionary\n",
    "df[\"obj_item\"]=df[\"obj\"].apply(lambda x: getStart(x))\n",
    "df[\"sub_item\"]=df[\"sub\"].apply(lambda x: getStart(x))\n",
    "df[\"assessplan_item\"]=df[\"assessplan\"].apply(lambda x: getStart(x))\n",
    "\n",
    "\n",
    "# Instead of the match content (section header) we want the dictionary to indicate which section is starting\n",
    "# For example, instead of \"HPI:\", we want it to say \"Subjective:\"\n",
    "# this code changes the keys as such so we can build an easily readable dictionary with all sections\n",
    "df[\"obj_item\"]=df[\"obj_item\"].apply(lambda x: replaceTitle(x,\"Objective\"))\n",
    "df[\"sub_item\"]=df[\"sub_item\"].apply(lambda x: replaceTitle(x,\"Subjective\"))\n",
    "df[\"assessplan_item\"]=df[\"assessplan_item\"].apply(lambda x: replaceTitle(x,\"Assess_Plan\"))\n",
    "\n",
    "# put all sections together into a list, then convert to an ordered dictionary\n",
    "df[\"ind_list\"] = df[[\"sub_item\",\"obj_item\",\"assessplan_item\"]].values.tolist()\n",
    "df[\"ind_list\"]=df[\"ind_list\"].apply(lambda x: createOrderedDict(x))\n",
    "\n",
    "# use ordered dictionary to extract the start and end index of each section\n",
    "df[\"sub_index\"]=df[\"ind_list\"].apply(lambda x: getStartEnd(x,\"Subjective\"))\n",
    "df[\"obj_index\"]=df[\"ind_list\"].apply(lambda x: getStartEnd(x,\"Objective\"))\n",
    "df[\"ap_index\"]=df[\"ind_list\"].apply(lambda x: getStartEnd(x,\"Assess_Plan\"))\n",
    "\n",
    "\n",
    "# CREATE COLUMNS CONTAINING THE START AND END INDEX OF EACH COMPONENT\n",
    "#Subjective\n",
    "df[\"sub_start\"]=df[\"sub_index\"].apply(lambda x: getStart(x))\n",
    "df[\"sub_end\"]=df[\"sub_index\"].apply(lambda x: getEnd(x))\n",
    "\n",
    "#Objective\n",
    "df[\"obj_start\"]=df[\"obj_index\"].apply(lambda x: getStart(x))\n",
    "df[\"obj_end\"]=df[\"obj_index\"].apply(lambda x: getEnd(x))\n",
    "\n",
    "#Assessment/Plan\n",
    "df[\"ap_start\"]=df[\"ap_index\"].apply(lambda x: getStart(x))\n",
    "df[\"ap_end\"]=df[\"ap_index\"].apply(lambda x: getEnd(x))\n",
    "\n",
    "\n",
    "# ASSIGN COMPONENTS TO SUBSTRING, USING NEWLY GENERATED INDEXES\n",
    "# assign subjective and objective\n",
    "df[\"Subjective\"]=df.apply(lambda row: row.note_text[row.sub_start:row.sub_end], axis=1)\n",
    "df[\"Objective\"]=df.apply(lambda row: row.note_text[row.obj_start:row.obj_end], axis=1)\n",
    "\n",
    "# assign assessment and plan the same text, as they are the same section here\n",
    "df[\"Assessment\"]=df.apply(lambda row: row.note_text[row.ap_start:row.ap_end], axis=1)\n",
    "df[\"Plan\"]=df.apply(lambda row: row.note_text[row.ap_start:row.ap_end], axis=1)\n",
    "\n",
    "# reset index and drop all the columns we no longer need\n",
    "df=df.reset_index()\n",
    "df=df.drop(columns=[\"index\",\"sub_item\",\"obj_item\",\"assessplan_item\",\"sub\",\"obj\",\"assessplan\",\"ind_list\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extraction of four section notes'''\n",
    "\n",
    "# create temporary DataFrame that has subjective, objective, and\n",
    "# both assessment and plan, treated as separate sections\n",
    "df2=notes_obj[notes_obj[\"contains_objective\"]]\n",
    "df2=df2[df2[\"contains_assessplan\"]==False]\n",
    "df2=df2[df2[\"contains_subjective\"]]\n",
    "df2=df2[df2[\"contains_assess\"]]\n",
    "df2=df2[df2[\"contains_plan\"]]\n",
    "print(\"percent of notes that have Objective, Subjective, and both Assessment and Plan, treated as separate sections: \"+str(df2.shape[0]/notes_obj.shape[0]))\n",
    "\n",
    "\n",
    "# extract the first item in the match dictionary\n",
    "df2[\"obj_item\"]=df2[\"obj\"].apply(lambda x: getStart(x))\n",
    "df2[\"sub_item\"]=df2[\"sub\"].apply(lambda x: getStart(x))\n",
    "df2[\"assess_item\"]=df2[\"assess\"].apply(lambda x: getStart(x))\n",
    "df2[\"plan_item\"]=df2[\"plan\"].apply(lambda x: getStart(x))\n",
    "\n",
    "# change the keys so we can build an easily readable dictionary with all sections\n",
    "df2[\"obj_item\"]=df2[\"obj_item\"].apply(lambda x: replaceTitle(x,\"Objective\"))\n",
    "df2[\"sub_item\"]=df2[\"sub_item\"].apply(lambda x: replaceTitle(x,\"Subjective\"))\n",
    "df2[\"assess_item\"]=df2[\"assess_item\"].apply(lambda x: replaceTitle(x,\"Assessment\"))\n",
    "df2[\"plan_item\"]=df2[\"plan_item\"].apply(lambda x: replaceTitle(x,\"Plan\"))\n",
    "\n",
    "# put all sections together into a list, then convert to an ordered dictionary\n",
    "df2[\"ind_list\"] = df2[[\"sub_item\",\"obj_item\",\"assess_item\",\"plan_item\"]].values.tolist()\n",
    "df2[\"ind_list\"]=df2[\"ind_list\"].apply(lambda x: createOrderedDict(x))\n",
    "\n",
    "# use ordered dictionary to extract the start and end index of each section\n",
    "df2[\"sub_index\"]=df2[\"ind_list\"].apply(lambda x: getStartEnd(x,\"Subjective\"))\n",
    "df2[\"obj_index\"]=df2[\"ind_list\"].apply(lambda x: getStartEnd(x,\"Objective\"))\n",
    "df2[\"as_index\"]=df2[\"ind_list\"].apply(lambda x: getStartEnd(x,\"Assessment\"))\n",
    "df2[\"p_index\"]=df2[\"ind_list\"].apply(lambda x: getStartEnd(x,\"Plan\"))\n",
    "\n",
    "\n",
    "# CREATE COLUMNS CONTAINING THE START AND END INDEX OF EACH COMPONENT\n",
    "#Subjective\n",
    "df2[\"sub_start\"]=df2[\"sub_index\"].apply(lambda x: getStart(x))\n",
    "df2[\"sub_end\"]=df2[\"sub_index\"].apply(lambda x: getEnd(x))\n",
    "\n",
    "#Objective\n",
    "df2[\"obj_start\"]=df2[\"obj_index\"].apply(lambda x: getStart(x))\n",
    "df2[\"obj_end\"]=df2[\"obj_index\"].apply(lambda x: getEnd(x))\n",
    "\n",
    "#Assessment\n",
    "df2[\"as_start\"]=df2[\"as_index\"].apply(lambda x: getStart(x))\n",
    "df2[\"as_end\"]=df2[\"as_index\"].apply(lambda x: getEnd(x))\n",
    "\n",
    "#Plan\n",
    "df2[\"p_start\"]=df2[\"p_index\"].apply(lambda x: getStart(x))\n",
    "df2[\"p_end\"]=df2[\"p_index\"].apply(lambda x: getEnd(x))\n",
    "\n",
    "# ASSIGN COMPONENTS TO SUBSTRING, USING NEWLY GENERATED INDEXES\n",
    "# assign subjective and objective\n",
    "df2[\"Subjective\"]=df2.apply(lambda row: row.note_text[row.sub_start:row.sub_end], axis=1)\n",
    "df2[\"Objective\"]=df2.apply(lambda row: row.note_text[row.obj_start:row.obj_end], axis=1)\n",
    "df2[\"Assessment\"]=df2.apply(lambda row: row.note_text[row.as_start:row.as_end], axis=1)\n",
    "df2[\"Plan\"]=df2.apply(lambda row: row.note_text[row.p_start:row.p_end], axis=1)\n",
    "\n",
    "\n",
    "# reset index and drop all the columns we no longer need\n",
    "df2=df2.reset_index()\n",
    "df2=df2.drop(columns=[\"sub\",\"obj\",\"assess\",\"plan\",\"index\",\"sub_item\",\"obj_item\",\"assess_item\",\"plan_item\",\"ind_list\"])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''combine all extracted notes'''\n",
    "\n",
    "# merge back together the \"three section\" notes and the four section notes\n",
    "# into one big dataframe of notes that have all four components extracted\n",
    "notes_extracted= pd.concat([df,df2])\n",
    "notes_extracted=notes_extracted.reset_index()\n",
    "print(\"percent of total notes identified to have SOAP structure: \"+str(notes_extracted.shape[0]/notes_obj.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get diagnoses codes\n",
    "\n",
    "def getTags(col):\n",
    "    '''detect # diagnoses'''\n",
    "    if col is None:\n",
    "        return None\n",
    "    r1 = re.findall(r\"((#+[a-zA-Z(_)]{1,}) ?((\\(([^)]+)\\))*)( ?(\\b[A-Za-z].*?\\b))*)\",col)\n",
    "    diag=[]\n",
    "    for i in r1:\n",
    "        diag.append(i[0])\n",
    "    return diag\n",
    "\n",
    "notes_extracted[\"diags\"]=notes_extracted[\"note_text\"].apply(lambda x:getTags(x))\n",
    "notes_extracted[\"diag_detected\"]=False\n",
    "notes_extracted[\"diag_detected\"]=np.where(notes_extracted[\"diags\"],True,notes_extracted[\"diag_detected\"])\n",
    "print(\"percent of notes identified to have SOAP structure that have diag codes: \"+str(notes_extracted['diag_detected'].sum()/notes_extracted.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save extracted notes to csv\n",
    "notes_extracted.to_csv(\"~/projects/PACE - MIDS Student Portfolio Capstone/data/notes_extracted.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ff6fb2b77b87bd4f4ae2e6ac66b39f4ad33ae5721d6883644090ce24ad0605a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
