{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems Tagging\n",
    "\n",
    "The purpose of the code in this notebook is to perform systems tagging for the extracted entities (e.g. instead of \"weakness\" we will have \"weakness (Motor)\"). The result is an indicator table with \"1\" indicating a symptom/disease is present in the note and \"0\" indicating its abscence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# import spacy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# import scispacy\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m \u001b[39mimport\u001b[39;00m displacy\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m PhraseMatcher\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Span\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/fake_notes_extracted_obj.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Import notes\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m notes \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mdata/fake_notes_extracted_obj.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peiningyang/dukeoit-clinical-notes/code/systems_tagging.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m notes\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown engine: \u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m (valid options are \u001b[39m\u001b[39m{\u001b[39;00mmapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[39m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[39mreturn\u001b[39;00m mapping[engine](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols\n\u001b[1;32m     50\u001b[0m \u001b[39m# open handles\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open_handles(src, kwds)\n\u001b[1;32m     52\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_handles\u001b[39m(\u001b[39mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m    223\u001b[0m         src,\n\u001b[1;32m    224\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    225\u001b[0m         encoding\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    226\u001b[0m         compression\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    227\u001b[0m         memory_map\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    228\u001b[0m         storage_options\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    229\u001b[0m         errors\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    230\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/io/common.py:701\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    697\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    700\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    702\u001b[0m             handle,\n\u001b[1;32m    703\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    704\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    705\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    706\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    707\u001b[0m         )\n\u001b[1;32m    708\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    710\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/fake_notes_extracted_obj.csv'"
     ]
    }
   ],
   "source": [
    "# Import notes\n",
    "notes = pd.read_csv(\"data/fake_notes_extracted_obj.csv\")\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy models\n",
    "nlp0 = spacy.load(\"en_core_sci_sm\")\n",
    "nlp1 = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing the notes to capture all forms of negation(e.g., deny: denies, denying)\n",
    "def lemmatize(note, nlp):\n",
    "    doc = nlp(note)\n",
    "    # lemmatize and tokenize notes\n",
    "    lemNote = [wd.lemma_ for wd in doc]\n",
    "    # join the lemmatized tokens back to text\n",
    "    lemNote = \" \".join(lemNote)\n",
    "    # replace double spaces to period to mark the end of sentences\n",
    "    lemNote = lemNote.replace(\"  \", \". \")\n",
    "    return lemNote\n",
    "\n",
    "\n",
    "#function to modify options for displacy NER visualization\n",
    "def get_entity_options():\n",
    "    # list of entities the model should detect\n",
    "    entities = [\"DISEASE\", \"SYMPTOM\", \"NEG_ENTITY\"]\n",
    "    # assign colors to different entity labels\n",
    "    colors = {'DISEASE': 'linear-gradient(180deg, #66ffcc, #abf763)', \n",
    "    'SYMPTOM': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', \n",
    "    \"NEG_ENTITY\":'linear-gradient(90deg, #ffff66, #ff6600)'}\n",
    "    options = {\"ents\": entities, \"colors\": colors}    \n",
    "    return options\n",
    "\n",
    "def add_label_pattern(nlp, label, patterns):\n",
    "    # add new label \n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    ner.add_label(label)\n",
    "\n",
    "    # Add custom patterns to new entity labels\n",
    "\n",
    "    #Create the EntityRuler\n",
    "    if not \"entity_ruler\" in nlp.pipe_names:\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    else:\n",
    "        ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "\n",
    "    #List of Entities and Patterns\n",
    "    for p in patterns:\n",
    "        pattern = [\n",
    "                        {\"label\": label, \"pattern\": p}\n",
    "                    ]\n",
    "\n",
    "        ruler.add_patterns(pattern)\n",
    "\n",
    "\n",
    "#adding a new pipeline component to identify negation\n",
    "def neg_model(nlp):\n",
    "\n",
    "    if not \"sentencizer\" in nlp.pipe_names:\n",
    "        nlp.add_pipe('sentencizer')\n",
    "\n",
    "    ts = termset(\"en_clinical\")\n",
    "    # add custom patterns for negations\n",
    "    ts.add_patterns(\n",
    "        {\n",
    "        \"preceding_negations\":[\"unable\", \"w/o\"],\n",
    "        \"following_negations\": [\"was negative\"]\n",
    "        }\n",
    "    )\n",
    "    if not \"negex\" in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"negex\", config={\"neg_termset\": ts.get_patterns()})\n",
    "    return nlp\n",
    "\n",
    "\n",
    "# Negspacy sets a new attribute e._.negex to True if a negative concept is encountered\n",
    "def negation_handling(nlp, note):\n",
    "    results = []\n",
    "    #sentence tokenizing based on delimeter \n",
    "    note = note.split(\".\") \n",
    "    #removing extra spaces at the begining and end of sentence\n",
    "    note = [n.strip() for n in note] \n",
    "    for t in note:\n",
    "        doc = nlp(t)\n",
    "        for e in doc.ents:\n",
    "            rs = str(e._.negex)\n",
    "            if rs == \"True\": \n",
    "                results.append(e.text)\n",
    "    return results\n",
    "\n",
    "#function to identify span objects of matched megative phrases from clinical note\n",
    "def match(nlp,terms,label):\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        matcher.add(label, None, *patterns)\n",
    "        return matcher\n",
    "\n",
    "\n",
    "#replacing the labels for identified negative entities\n",
    "def overwrite_ent_lbl(matcher, doc):\n",
    "    matches = matcher(doc)\n",
    "    seen_tokens = set()\n",
    "    new_entities = []\n",
    "    entities = doc.ents\n",
    "    for match_id, start, end in matches:\n",
    "        if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "            new_entities.append(Span(doc, start, end, label=match_id))\n",
    "            entities = [\n",
    "                e for e in entities if not (e.start < end and e.end > start)\n",
    "            ]\n",
    "            seen_tokens.update(range(start, end))\n",
    "    doc.ents = tuple(entities) + tuple(new_entities)\n",
    "    return doc\n",
    "\n",
    "def negation_visualization(note, visualize=True):\n",
    "    lem_clinical_note = lemmatize(note, nlp0)\n",
    "    doc = nlp1(lem_clinical_note)\n",
    "    options = get_entity_options()\n",
    "    #list of negative concepts from clinical note identified by negspacy\n",
    "    results0 = negation_handling(nlp1, lem_clinical_note, neg_model)\n",
    "    matcher = match(nlp1, results0,\"NEG_ENTITY\")\n",
    "    #doc0: new doc object with added \"NEG_ENTITY label\"\n",
    "    doc0 = overwrite_ent_lbl(matcher,doc)\n",
    "    #visualizing identified Named Entities in clinical input text \n",
    "    if visualize:\n",
    "        displacy.render(doc0, style='ent', options=options)\n",
    "    # output dataframe\n",
    "    doc_0 = nlp1(doc0)\n",
    "    entities = [(e.label_,e.text) for e in doc_0.ents]\n",
    "\n",
    "    # Find systems\n",
    "    systems = re.findall(r\"[A-Za-z]+:\", note)\n",
    "\n",
    "    # Split strings by systems\n",
    "    split_notes = re.split(r\"[A-Za-z]+:\", note)\n",
    "\n",
    "    # Check if entity belongs to a specific system and tag the system to the entity\n",
    "    tagged_words = []\n",
    "    for word in doc_0.ents:\n",
    "        for i in range(len(split_notes)):\n",
    "            if str(word) in split_notes[i]:\n",
    "                tagged = str(word) + \" (\" + systems[i-1][:-1] + \")\"\n",
    "                tagged_words.append(tagged)\n",
    "\n",
    "    entities = [(e.label_) for e in doc_0.ents]\n",
    "    temp = [entities, tagged_words]\n",
    "    df = pd.DataFrame(temp).transpose()\n",
    "    df.columns = ['Entity', 'Identified']\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing code on the first note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_visualization(notes[\"Objective_const\"][0], False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add customized patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add patterns to existing model\n",
    "# manually add more to this list\n",
    "patterns = [\n",
    "    \"murmur\",\n",
    "    \"rub\",\n",
    "    \"rale\", \n",
    "    \"gallop\",\n",
    "    \"LAD\",\n",
    "    \"JVD\",\n",
    "    \"jvp\",\n",
    "    \"s3\",\n",
    "    \"s4\",\n",
    "    \"lymphadenopathy\",\n",
    "    \"focal deficit\",\n",
    "    \"tachycardic\",\n",
    "    \"congestion\",\n",
    "    \"rhonchi\",\n",
    "    \"poor tone\",\n",
    "    \"ck\" , \n",
    "    \"tsh\" ,\n",
    "    \"tpn-2 wln\",\n",
    "    \"intact downward gaze\",\n",
    "    \"neck flexion\",\n",
    "    \"lesion\", \n",
    "    \"click\",\n",
    "    \"carotid bruit\",\n",
    "    \"varicosity\",\n",
    "    \"nodule\",\n",
    "    \"deformity\",\n",
    "    \"eruption\",\n",
    "    \"suicidal\",\n",
    "    \"drainage\",\n",
    "    \"crackle\",\n",
    "    \"retraction\",\n",
    "    \"distention\",\n",
    "    \"distress\"\n",
    "]\n",
    "\n",
    "# add label to model\n",
    "add_label_pattern(nlp1, \"SYMPTOM\", patterns)\n",
    "nlp1 = neg_model(nlp1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run function on all notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function for first 100 notes and input into new dataframe\n",
    "new_df = pd.DataFrame()\n",
    "for i, note in enumerate(notes['Objective_const'][:100]):\n",
    "    if (i+1) % 500 == 0:\n",
    "        print(i+1, 'notes completed')\n",
    "    neg_df = negation_visualization(note, False)\n",
    "    neg_df['note_num'] = i\n",
    "    new_df = pd.concat([new_df, neg_df], axis=0)\n",
    "    new_df = new_df.drop_duplicates([\"Identified\", \"note_num\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create indicator table from extracted entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['positive'] = 1\n",
    "new_df.loc[new_df['Entity']=='NEG_ENTITY', 'positive'] = 0\n",
    "result_df = new_df.pivot_table(index='note_num',columns=[\"Identified\"], values='positive')\n",
    "result_df.fillna(0, inplace=True)\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
